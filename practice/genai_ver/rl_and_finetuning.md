# ファインチューニングと強化学習の比較まとめ

## 全体像：モデルを「育てる」3ステップ

```
事前学習
  ↓
ファインチューニング  ← 知識・形式・口調を教え込む
  ↓
強化学習 (RLHF)     ← 振る舞い・誠実さを微調整する
```

この3段階はすべて「モデルのパラメータを変える」話。
ChatGPT / Claude / Gemini など主要LLMは全部この3段階を踏んでいる。

---

## 料理の例え

| 手法 | 例え | 学習のさせ方 |
|---|---|---|
| 事前学習 | 義務教育 | 言葉・計算・一般常識を学ぶ。まだ料理はできない |
| ファインチューニング | 料理学校 | 「このレシピ通りに作りなさい」と教わる → 基礎スキルを身につける |
| 強化学習 | 実地研修 | 客に出した料理に「美味しい」「塩辛い」と評価される → 好まれる味付けを身につける |

---

## ファインチューニング

### 仕組み
- 「入力」と「正解」のペアで学習（Token Level）
- 正解データと一言一句合わせようとする
- **白黒つけがち**：特定の正解に収束させる

### 得意なこと
- 新しい知識の注入（専門用語・社内用語）
- JSONなど特殊フォーマットの出力
- 口調・キャラクター（ペルソナ）の固定
- 特定タスクへの特化（メール件名生成、OCR結果の構造化など）

### ユースケースの判断基準
> 「こう答えてほしい」が明確に言語化できるならファインチューニング

---

## 強化学習 (RLHF)

### 仕組み
- 回答の「比較」や「採点」で学習（Sentence Level）
- 文章全体に対してスコア（報酬）を与える
- **グラデーションで学習**：より良い方向に継続的に調整

```
回答A: 「山田太郎です」         → スコア低（実は間違い）
回答B: 「わかりません」         → スコア高（正直）
回答C: 「〇〇と言われていますが確認できません」 → スコア高
```

### 得意なこと
- 嘘（ハルシネーション）の抑制
- 有害な回答の回避
- 「もっと丁寧に」など曖昧な指示への対応

### なぜハルシネーション抑制に効くのか
- ファインチューニングは「それっぽい答えを生成する」ことを学んでしまう
- 強化学習では「不確かなことは不確かと言う」行動に高報酬を与えることで、正直な振る舞いを学習させられる

### 主なユースケース
| ユースケース | 報酬の基準 |
|---|---|
| チャットボットの品質向上 | 人間の好み・安全性 |
| コード生成 | 実行結果（成功/失敗） |
| 数学・論理推論 | 正誤 |
| 要約・翻訳 | 品質評価 |

> **「正解が一意に決まらないもの」ほどRLが活きる**

---

## ファインチューニング vs 強化学習

| 比較項目 | ファインチューニング | 強化学習 |
|---|---|---|
| 主な目的 | 指示従順性の獲得・知識注入 | 人間との調和・振る舞いの調整 |
| データ形式 | 入力と正解のペア | 回答の比較・採点 |
| 学習の粒度 | Token Level | Sentence Level |
| 正解の性質 | 一意に決まる | 人間が判断する |
| イメージ | 白黒・収束 | グラデーション・微調整 |

---

## RAG（ベクトル検索）との違い

### レイヤーが根本的に違う

```
【学習フェーズ】モデルのパラメータを変える
  事前学習 → ファインチューニング → 強化学習

【推論フェーズ】モデルは変えず外から情報を渡す
  RAG（ベクトル検索） / プロンプトエンジニアリング
```

### ハルシネーション抑制の観点での比較

| アプローチ | 手段 | 効果 |
|---|---|---|
| 強化学習 | モデル自体を「正直に」鍛える | 「知らない」と言えるようになる |
| RAG | 正しい情報を外から与える | 知識不足による誤答を減らす |

→ **両方組み合わせるのが現実的な解**

---

## コスト観点での使い分け

```
プロンプトエンジニアリング  低コスト・すぐできる
  ↓
RAG（ベクトル検索）        中コスト・知識補完に強い
  ↓
ファインチューニング        高コスト・形式や知識の注入
  ↓
強化学習                  最高コスト・振る舞いの根本調整
```

> まず外側から試して、どうにもならなければ内側（モデル自体）に手を入れる

---

## なぜLLMを使う側の企業は強化学習をやらないのか

### 理由1：コストが桁違い
```
ファインチューニング: 数万円〜数十万円
強化学習:           数億円〜数十億円
```

### 理由2：人間の評価者が大量に必要
RLHFは「回答AとBどちらが良い？」を人間が何万回も判断する必要がある。

### 理由3：モデルのパラメータにアクセスできない
```
OpenAI / Anthropic / Google → パラメータを持っている → 強化学習できる
一般企業                    → APIを叩くだけ → パラメータに触れない
```

---

## SLMになると変わること

| | LLM（API利用） | SLM（自社運用） |
|---|---|---|
| 強化学習 | ほぼ無理 | 技術的には可能 |
| 現実的な戦略 | プロンプト＋RAG | ファインチューニング重視 |
| 強みを活かす方向 | 汎用性 | 特化・軽量・コスト |

### SLMでもファインチューニング重視な理由
- SLMは最初から能力が低いため、微調整より知識・スキル不足の解消が先決
- まず用途を絞り、特化タスクのデータを大量に用意してファインチューニングするのが現実的

---

## まとめ：強化学習の本質

> **強化学習 = 最後の「仕上げ・微調整」**

- 強化学習単体では意味がない（土台が必要）
- 「最後の1割を磨く」ために一番コストがかかる
- だからこそ、LLMを使う側の企業はやらない
